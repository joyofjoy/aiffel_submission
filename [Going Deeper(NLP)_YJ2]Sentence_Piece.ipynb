{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f28327d",
   "metadata": {},
   "source": [
    "## 2-5. 프로젝트: SentencePiece 사용하기\n",
    "라이브러리 버전을 확인해 봅니다.  \n",
    "사용할 라이브러리 버전을 둘러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652dcb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.22.2\n",
      "3.4.3\n",
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import konlpy\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(plt.__version__)\n",
    "print(konlpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b778e8",
   "metadata": {},
   "source": [
    "### Step 1. SentencePiece 설치하기\n",
    "SentencePiece는 Google에서 제공하는 오픈소스 기반 Sentence Tokenizer/Detokenizer 로서, BPE와 unigram 2가지 subword 토크나이징  \n",
    "모델 중 하나를 선택해서 사용할 수 있도록 패키징한 것입니다. 아래 링크의 페이지에서 상세한 내용을 파악할 수 있습니다.\n",
    "\n",
    "google/sentencepiece: https://github.com/google/sentencepiece  \n",
    "위 페이지의 서두에서도 언급하고 있듯, SentencePiece는 딥러닝 자연어처리 모델의 앞부분에 사용할 목적으로 최적화되어 있는데,  \n",
    "최근 pretrained model들이 거의 대부분 SentencePiece를 tokenizer로 채용하면서 사실상 표준의 역할을 하고 있습니다.\n",
    "\n",
    "다음과 같이 설치를 진행합니다. SentencePiece는 python에서 쓰라고 만들어진 라이브러리는 아니지만 편리한 파이썬 wrapper를  \n",
    "아래와 같이 제공하고 있습니다.\n",
    "\n",
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1053e68e",
   "metadata": {},
   "source": [
    "### Step 2. SentencePiece 모델 학습\n",
    "앞서 배운 tokenize() 함수를 기억하나요? 다시 한번 상기시켜드릴게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2256c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(corpus):  # corpus: Tokenized Sentence's List\n",
    "#     tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "#     tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "#     tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "#     return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f3a46",
   "metadata": {},
   "source": [
    "위와 같이 tf.keras.preprocessing.text.Tokenizer에 corpus를 주고 tokenizer.fit_on_texts(corpus)을 하면 토크나이저 내부적으로  \n",
    "단어사전과 토크나이저 기능을 corpus에 맞춤형으로 자동 생성해 주는 것입니다.\n",
    "\n",
    "그럼 이를 위해서 SentencePiece 모델을 학습하는 과정을 거쳐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d45021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "# import os\n",
    "# temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "# vocab_size = 8000\n",
    "\n",
    "# with open(temp_file, 'w') as f:\n",
    "#     for row in filtered_corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
    "#         f.write(str(row) + '\\n')\n",
    "\n",
    "# spm.SentencePieceTrainer.Train(\n",
    "#     '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)    \n",
    "# )\n",
    "# #위 Train에서  --model_type = 'unigram'이 디폴트 적용되어 있습니다. --model_type = 'bpe' 로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "# !ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29502aa2",
   "metadata": {},
   "source": [
    "위 코드를 실행하면 정상적으로 SentencePiece 모델 학습이 완료된 후 korean_spm.model 파일과 korean_spm.vocab vocabulary  \n",
    "파일이 생성되었음을 확인할 수 있습니다.\n",
    "\n",
    "그럼 이렇게 학습된 SentencePiece 모델을 어떻게 활용하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "287f05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = spm.SentencePieceProcessor()\n",
    "# s.Load('korean_spm.model')\n",
    "\n",
    "# # SentencePiece를 활용한 sentence -> encoding\n",
    "# tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "# print(tokensIDs)\n",
    "\n",
    "# # SentencePiece를 활용한 sentence -> encoded pieces\n",
    "# print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# # SentencePiece를 활용한 encoding -> sentence 복원\n",
    "# print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec685bce",
   "metadata": {},
   "source": [
    "어떻습니까? SentencePiece의 토크나이징 실력이 괜찮은 것 같습니다.\n",
    "\n",
    "### Step 3. Tokenizer 함수 작성\n",
    "우리는 위에서 훈련시킨 SentencePiece를 활용하여 위 함수와 유사한 기능을 하는 sp_tokenize() 함수를 정의할 겁니다.  \n",
    "하지만 SentencePiece가 동작하는 방식이 단순 토큰화와는 달라 완전히 동일하게는 정의하기 어렵습니다. 그러니 아래 조건을  \n",
    "만족하는 함수를 정의하도록 하습니다.\n",
    "\n",
    "1) 매개변수로 토큰화된 문장의 list를 전달하는 대신 온전한 문장의 list 를 전달합니다.\n",
    "\n",
    "2) 생성된 vocab 파일을 읽어와 { <word> : <idx> } 형태를 가지는 word_index 사전과 { <idx> : <word>} 형태를 가지는 index_word 사전을  \n",
    "    생성하고 함께 반환합니다.\n",
    "\n",
    "3) 리턴값인 tensor 는 앞의 함수와 동일하게 토큰화한 후 Encoding된 문장입니다. 바로 학습에 사용할 수 있게 Padding은 당연히 해야겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e328a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sp_tokenize(s, corpus):\n",
    "\n",
    "#     tensor = []\n",
    "\n",
    "#     for sen in corpus:\n",
    "#         tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "#     with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "#         vocab = f.readlines()\n",
    "\n",
    "#     word_index = {}\n",
    "#     index_word = {}\n",
    "\n",
    "#     for idx, line in enumerate(vocab):\n",
    "#         word = line.split(\"\\t\")[0]\n",
    "\n",
    "#         word_index.update({idx:word})\n",
    "#         index_word.update({word:idx})\n",
    "\n",
    "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "#     return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a52707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sp_tokenize(s, corpus) 사용예제\n",
    "\n",
    "# my_corpus = ['나는 밥을 먹었습니다.', '그러나 여전히 ㅠㅠ 배가 고픕니다...']\n",
    "# tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "# print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c8d95",
   "metadata": {},
   "source": [
    "### Step 4. 네이버 영화리뷰 감정 분석 문제에 SentencePiece 적용해 보기\n",
    "네이버 영화리뷰 감정 분석 태스크가 있습니다. 한국어로 된 corpus를 다루어야 하므로 주로 KoNLPy에서 제공하는 형태소 분석기를  \n",
    "사용하여 텍스트를 전처리해서 RNN 모델을 분류기로 사용하게 되는데요.\n",
    "\n",
    "만약 이 문제에서 tokenizer를 SentencePiece로 바꾸어 다시 풀어본다면 더 성능이 좋아질까요? 비교해 보는 것도 흥미로울 것입니다.\n",
    "\n",
    "- 네이버 영화리뷰 감정 분석 코퍼스에 SentencePiece를 적용시킨 모델 학습하기\n",
    "- 학습된 모델로 sp_tokenize() 메소드 구현하기\n",
    "- 구현된 토크나이저를 적용하여 네이버 영화리뷰 감정 분석 모델을 재학습하기\n",
    "- KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기\n",
    "- (보너스) SentencePiece 모델의 model_type, vocab_size 등을 변경해 가면서 성능 개선 여부 확인하기\n",
    "\n",
    "Word Vector는 활용할 필요가 없습니다. 활용이 가능하지도 않을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469ee09",
   "metadata": {},
   "source": [
    "### 1단계: SentencePiece 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a426cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c8d2b",
   "metadata": {},
   "source": [
    "### 2단계: SentencePiece 모델화 및 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00802cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko'\n",
    "\n",
    "with open(path_to_file, \"r\") as f: \n",
    "    raw = f.read().splitlines() # 줄 단위로 읽어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58057ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107/1943799563.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sentence_length = np.zeros((max_len), dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "min_len = 999\n",
    "max_len = 0\n",
    "\n",
    "for sen in raw:\n",
    "    length = len(sen)\n",
    "    \n",
    "    # 문장 최소 길이 찾기\n",
    "    if min_len > length: \n",
    "        min_len = length\n",
    "    \n",
    "    # 문장 최대 길이 찾기\n",
    "    if max_len < length: \n",
    "        max_len = length\n",
    "        \n",
    "    \n",
    "sentence_length = np.zeros((max_len), dtype=np.int)\n",
    "\n",
    "cleaned_corpus = list(set(raw))  # set를 사용해서 중복을 제거합니다.\n",
    "\n",
    "filtered_corpus = [s for s in cleaned_corpus if (len(s) < max_len) & (len(s) >= min_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44a8a485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 77590 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5079802\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1319\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 77590 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 176795 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 77590\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 241182\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 241182 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=93808 obj=14.8611 num_tokens=530931 num_tokens/piece=5.65976\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=83222 obj=13.5216 num_tokens=533486 num_tokens/piece=6.4104\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=62408 obj=13.5582 num_tokens=554869 num_tokens/piece=8.89099\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=62357 obj=13.5154 num_tokens=555301 num_tokens/piece=8.90519\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46766 obj=13.6977 num_tokens=583543 num_tokens/piece=12.4779\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46766 obj=13.6548 num_tokens=583593 num_tokens/piece=12.479\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35074 obj=13.8914 num_tokens=614352 num_tokens/piece=17.5159\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35074 obj=13.8406 num_tokens=614386 num_tokens/piece=17.5169\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26305 obj=14.1327 num_tokens=646390 num_tokens/piece=24.5729\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26305 obj=14.0776 num_tokens=646424 num_tokens/piece=24.5742\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19728 obj=14.4108 num_tokens=680165 num_tokens/piece=34.4771\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19728 obj=14.3488 num_tokens=680202 num_tokens/piece=34.479\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14796 obj=14.7201 num_tokens=715226 num_tokens/piece=48.3391\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14796 obj=14.6487 num_tokens=715228 num_tokens/piece=48.3393\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11097 obj=15.0839 num_tokens=751803 num_tokens/piece=67.7483\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11097 obj=15.0033 num_tokens=751803 num_tokens/piece=67.7483\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=15.3984 num_tokens=781516 num_tokens/piece=88.8086\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=15.3245 num_tokens=781525 num_tokens/piece=88.8097\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 376725 Mar 23 03:11 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 146094 Mar 23 03:11 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "# vacab size\n",
    "vocab_size = 8000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = 'unigram'이 디폴트 적용되어 있습니다. --model_type = 'bpe' 로 옵션을 주어 변경할 수 있습니다.\n",
    "# input: 입력 corpus\n",
    "# prefix: 저장할 모델 이름\n",
    "# vocab_size: vocab 개수 (기본 8,000)\n",
    "\n",
    "\n",
    "!ls -l korean_spm*\n",
    "\n",
    "# SentencePiece 모델 학습이 완료된 후 koreanspm.model 파일과 koreanspm.vocab vocabulary 파일이 생성되었음을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dabe6992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1255, 11, 304, 7, 3606, 11, 285, 38, 3]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "# 학습된 SentencePiece 활용하기\n",
    "\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2999a98",
   "metadata": {},
   "source": [
    "### 3단계: sp_tokenize() 함수를 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21e6c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메서드 구현\n",
    "import tensorflow as tf\n",
    "# s = spm.SentencePieceProcessor()\n",
    "def sp_tokenize(s, corpus):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen)) # 숫자로 임베딩\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # 패딩까지 // 세 문장 중 가장 긴 것을 토대로 패딩?\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c8dc9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1255   11  833    7 2342  285   38    3    0    0    0    0    0    0]\n",
      " [1957 5607    5    4 7975 2012    3    0    0    0    0    0    0    0]\n",
      " [ 108 1658  101    4    0  470   11    4   14    0 2002    3    3    3]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#sp_tokenize(s, corpus) 사용예제\n",
    "\n",
    "my_corpus = ['아버지가 방에 들어가신다.','나는 밥을 먹었습니다.', '그러나 여전히 ㅠㅠ 배가 고픕니다...']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(tensor)\n",
    "# print(word_index)\n",
    "# print(index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25aa4bf",
   "metadata": {},
   "source": [
    "### 4단계 네이버 데이터 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2348a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nsmc'...\n",
      "remote: Enumerating objects: 14763, done.\u001b[K\n",
      "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
      "Receiving objects: 100% (14763/14763), 56.19 MiB | 17.27 MiB/s, done.\n",
      "Resolving deltas: 100% (1749/1749), done.\n",
      "Updating files: 100% (14737/14737), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/e9t/nsmc.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f44180bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  6270596                                                굳 ㅋ      1\n",
       "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_table(\"nsmc/\"+\"ratings_train.txt\")\n",
    "test_data = pd.read_table(\"nsmc/\"+\"ratings_test.txt\")\n",
    "\n",
    "display(train_data.head())\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638efe53",
   "metadata": {},
   "source": [
    "### 5단계 : 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d14e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 유니크 : 146182\n",
      "타겟데이터 유니크 :  2\n"
     ]
    }
   ],
   "source": [
    "print(\"학습데이터 유니크 :\",train_data['document'].nunique()) #  약 4,000개의 중복 샘플이 존재\n",
    "print(\"타겟데이터 유니크 : \",train_data['label'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83a3f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복데이터 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # drop_duplicates 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b314631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# document에 null 값이 있는지 확인\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3ef47d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Null값 제거\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "414c67de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107/789371173.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 한글과 공백을 제외하고 모두 제거\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 한글과 공백을 제외하고 모두 제거\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec742e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146182\n"
     ]
    }
   ],
   "source": [
    "# null 데이터 삭제\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa09b523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 48852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107/3707744765.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
      "/tmp/ipykernel_107/3707744765.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n"
     ]
    }
   ],
   "source": [
    "# 테스트데이터에도 동일한 과정 진행\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16f84705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최단 길이: 0\n",
      "문장의 최장 길이: 140\n",
      "문장의 평균 길이: 32\n",
      "sentence_length :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107/2857138701.py:27: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sentence_length = np.zeros((max_len), dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYlElEQVR4nO3de5BcZZ3G8e/DHUVJgIghk3WioC5YihgF1N2lQEkCQlhL2bisBs1W1i3cRQtFAlteAYO6oLgIRkEQkYt4ISKKEbB2vSETlXCJkVGCSbgkkISbigR++8d5Gw9Dd7on09N9ut/nUzWVPu85ffrX7/Q8/Z73nO4oIjAzszxs1e0CzMyscxz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibtZmkQUkhaZs27vMYST9o4/5uk3RQuv0RSV9t475PlvSldu3P2suh3+ckvV7STyU9KGm9pJ9IenUb9nuspB+3o8Z2krRS0ht66TElXSjpL5IeTj+3SvqEpJ1r20TEJRFxaIv7OrXZdhGxT0T8aEtrLj3eQZJWj9j36RHxr2Pdt40Ph34fk/Rc4Grgc8AuwBTgo8Bj3azL6vpkRDwHmAS8EzgA+ImkZ7fzQdp59GG9yaHf314MEBGXRsQTEfGniPhBRCyrbSDpXZKWS9og6VpJLyitC0nvlnSHpI2SzlHhb4HzgAMlPSJpY9p+e0mflvQHSfdJOk/SjmndQZJWSzpB0lpJ90h6Z+mxdpT035LuSkclPy7d94B0tLJR0s21aYnRkLSVpJMk/U7SA5KukLRLWlebjpmbar9f0ikjarso9dFySSfWRreSLgb+BvhO6osTSw97TL39bU5E/DkibgKOBHaleAN42pFV+h2clfrxIUm3SHqZpPnAMcCJqZbvpO1XSvqgpGXAo5K2qXN0soOky9ORxi8lvaL0/EPSnqXlCyWdmt6QvgfskR7vEUl7aMR0kaQjVUwnbZT0o/T6qa1bKen9kpal3/vlknZopa9syzj0+9tvgSdSYM2SNLG8UtJs4GTgzRQjzP8DLh2xjzcBrwZeDhwNzIiI5cC7gZ9FxE4RMSFtu5DijWZfYE+KI4sPlfb1fGDn1D4POKdU06eBVwGvpTgqORF4UtIU4LvAqan9/cA3JE0aZV/8B3AU8A/AHsAG4JwR27weeAlwCPChUjh9GBgEXgi8EfiX2h0i4u3AH4AjUl98soX9NRURDwNLgL+rs/pQ4O8p+npnit/LAxGxCLiE4qhhp4g4onSftwGHAxMiYlOdfc4Gvk7Rx18Dvi1p2yY1PgrMAu5Oj7dTRNxd3kbSiyleU++leI1dQ/EGuV1ps6OBmcA0itfZsZt7XBsbh34fi4iHKIIngC8C6yQtlrR72uTdwCciYnkKgtOBfcujfWBhRGyMiD8AN1AE+jNIEjAfeF9ErE+hdTowp7TZ48DHIuLxiLgGeAR4iaStgHcBx0fEmnRU8tOIeIwiYK+JiGsi4smIWAIMAYeNsjveDZwSEavTfj8CvEVPn+74aDoauhm4GaiNdo8GTo+IDRGxGji7xcdstL9W3U0RwiM9DjwHeCmg9Pu7p8m+zo6IVRHxpwbrl0bElRHxOHAmsAPFFNNY/RPw3YhYkvb9aWBHijf3cm13R8R64Ds0eI1Zezj0+1wKhGMjYgB4GcUo9zNp9QuAz6bD7o3AekAUI/Gae0u3/wjs1OChJgHPApaW9vf91F7zwIhRZm1/u1GEzO/q7PcFwFtr+0z7fT0weXPPu8F+vlXax3LgCWD30jaNnusewKrSuvLtzWm17xqZQvE7eZqIuB74H4ojlbWSFqk4f7M5zWp+an1EPAmspnjeY7UHcNeIfa9iy15j1gYO/YxExG+ACynCH4o/vn+LiAmlnx0j4qet7G7E8v3An4B9SvvaOSJa+QO+H/gz8KI661YBF4+o8dkRsbCF/Y7cz6wR+9khIta0cN97gIHS8tQR69v+VbWSdgLeQDHl9gwRcXZEvArYm2Ka5wNNamlW41PPKR15DVAcaUARxM8qbfv8Uez3boo33Nq+lR6rlX63ceDQ72OSXppOnA6k5akUc7s/T5ucByyQtE9av7Okt7a4+/uAgdrcbBrBfRE4S9Lz0v6mSJrRbEfpvhcAZ6YTgVtLOlDS9sBXgSMkzUjtO6g4KTywmV1um7ar/WyTnutptakrSZPSOY1WXEHRTxPTOYb31OmLF7a4r81ScTL8VcC3Kc47fLnONq+WtH+ac3+U4g3zyTHW8ipJb0599V6KK7xqr5NfA/+c+n8mxXmRmvuAXVW6vHSEK4DDJR2S6j0h7buVgYWNA4d+f3sY2B+4UdKjFH/Et1L84RER3wLOAC6T9FBaN6vFfV8P3AbcK+n+1PZBYBj4edrfDylOZLbi/cAtwE0UUxpnAFtFxCqKk4wnA+soRuwfYPOv3WsojjpqPx8BPgssBn4g6WGKvti/xdo+RjHdcWd6Tlfy9MtePwH8V5o6en+L+xzpxFTXA8BXgKXAa9PJ0pGeS/EGu4Fi6uQB4FNp3fnA3qmWb4/i8a+imH/fALwdeHOagwc4HjgC2EhxddBT+01Hj5cCv0+P+bQpoYhYQXFe5nMUR3RHUJz0/ssoarM2kv8TFbPRkfTvwJyI+IemG5tVjEf6Zk1ImizpdSqu9X8JxZHSt7pdl9mW8KfzzJrbDvgCxXXkG4HLgM93syCzLeXpHTOzjHh6x8wsI5We3tltt91icHCw22WYmfWUpUuX3h8Rdb+qpNKhPzg4yNDQULfLMDPrKZLuarTO0ztmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6LfB4EnfZfCk73a7DDOzphz6ZmYZcei3kUf8ZlZ1Dn0zs4w49M3MMuLQHwee5jGzqqr09+lXmUPdzHqRR/pmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlpOfQlbS3pV5KuTsvTJN0oaVjS5ZK2S+3bp+XhtH6wtI8FqX2FpBltfzZmZrZZoxnpHw8sLy2fAZwVEXsCG4B5qX0esCG1n5W2Q9LewBxgH2Am8HlJW4+tfDMzG42WQl/SAHA48KW0LOBg4Mq0yUXAUen27LRMWn9I2n42cFlEPBYRdwLDwGva8BzMzKxFrY70PwOcCDyZlncFNkbEprS8GpiSbk8BVgGk9Q+m7Z9qr3Ofp0iaL2lI0tC6detafyZmZtZU09CX9CZgbUQs7UA9RMSiiJgeEdMnTZrUiYccN/46BjOrmla+huF1wJGSDgN2AJ4LfBaYIGmbNJofANak7dcAU4HVkrYBdgYeKLXXlO/T18rBv3Lh4V2sxMxy13SkHxELImIgIgYpTsReHxHHADcAb0mbzQWuSrcXp2XS+usjIlL7nHR1zzRgL+AXbXsmZmbW1Fi+cO2DwGWSTgV+BZyf2s8HLpY0DKyneKMgIm6TdAVwO7AJOC4inhjD45uZ2SiNKvQj4kfAj9Lt31Pn6puI+DPw1gb3Pw04bbRFVonn6M2sl/kTuWZmGXHom5llxKFvZpYRh76ZWUYc+h3mD2yZWTc59M3MMuLQNzPLiEPfzCwjDn0zs4w49LvEJ3TNrBsc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh36X+Xp9M+skh76ZWUYc+mZmGRnVf4yeM0/BmFk/8EjfzCwjDv2K8AldM+sEh76ZWUYc+mZmGXHom5llxKFfMZ7bN7Px5NA3M8uIr9OvqPJof+XCw7tYiZn1E4/0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ7+H+INbZjZWDn0zs4z4w1k9wKN7M2sXj/TNzDLSNPQl7SDpF5JulnSbpI+m9mmSbpQ0LOlySdul9u3T8nBaP1ja14LUvkLSjHF7VmZmVlcrI/3HgIMj4hXAvsBMSQcAZwBnRcSewAZgXtp+HrAhtZ+VtkPS3sAcYB9gJvB5SVu38bmYmVkTTUM/Co+kxW3TTwAHA1em9ouAo9Lt2WmZtP4QSUrtl0XEYxFxJzAMvKYdTyI3vorHzLZUS3P6kraW9GtgLbAE+B2wMSI2pU1WA1PS7SnAKoC0/kFg13J7nfuUH2u+pCFJQ+vWrRv1EzIzs8ZaCv2IeCIi9gUGKEbnLx2vgiJiUURMj4jpkyZNGq+H6Sse+ZtZq0Z19U5EbARuAA4EJkiqXfI5AKxJt9cAUwHS+p2BB8rtde5jZmYd0PQ6fUmTgMcjYqOkHYE3UpycvQF4C3AZMBe4Kt1lcVr+WVp/fUSEpMXA1ySdCewB7AX8os3Pp+2qPIKucm1mVk2tfDhrMnBRutJmK+CKiLha0u3AZZJOBX4FnJ+2Px+4WNIwsJ7iih0i4jZJVwC3A5uA4yLiifY+HTMz25ymoR8Ry4BX1mn/PXWuvomIPwNvbbCv04DTRl+mmZm1gz+Ra2aWEYe+mVlGHPpmZhlx6JuZZcSh36f8gS0zq8ffp99HHPJm1oxH+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxJdsNuDLH82sH3mkb2aWEYe+mVlGHPpmZhlx6JuZZcSh3+f8xWtmVubQz4TD38zAoW9mlhWHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZcehnxtfrm+XNoW9mlhGHvplZRhz6ZmYZ8f+cNYLnu82sn3mkb2aWEYe+mVlGPL2TqXrTWCsXHt6FSsyskzzSNzPLiEPfzCwjDn17ij+ta9b/HPpmZhlx6JuZZaRp6EuaKukGSbdLuk3S8al9F0lLJN2R/p2Y2iXpbEnDkpZJ2q+0r7lp+zskzR2/p2VmZvW0csnmJuCEiPilpOcASyUtAY4FrouIhZJOAk4CPgjMAvZKP/sD5wL7S9oF+DAwHYi0n8URsaHdT2q0PI/9dLX+8CWcZv2n6Ug/Iu6JiF+m2w8Dy4EpwGzgorTZRcBR6fZs4CtR+DkwQdJkYAawJCLWp6BfAsxs55MxM7PNG9WcvqRB4JXAjcDuEXFPWnUvsHu6PQVYVbrb6tTWqH3kY8yXNCRpaN26daMpz8zMmmj5E7mSdgK+Abw3Ih6S9NS6iAhJ0Y6CImIRsAhg+vTpbdmntYc/xWvW+1oa6UvaliLwL4mIb6bm+9K0Denftal9DTC1dPeB1Nao3SrK1+2b9Z9Wrt4RcD6wPCLOLK1aDNSuwJkLXFVqf0e6iucA4ME0DXQtcKikielKn0NTm1Wcw9+sf7QyvfM64O3ALZJ+ndpOBhYCV0iaB9wFHJ3WXQMcBgwDfwTeCRAR6yV9HLgpbfexiFjfjidhZmataRr6EfFjQA1WH1Jn+wCOa7CvC4ALRlOgmZm1jz+Ra2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+jYmv4TfrLQ59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy0jL/3NWP/L15WaWG4/0zcwy4tC3tvAnc816Q5bTOw4nM8uVR/pmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6Ftb+SuWzarNoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZ2abbBXSSrx83s9w1HelLukDSWkm3ltp2kbRE0h3p34mpXZLOljQsaZmk/Ur3mZu2v0PS3PF5OmZmtjmtTO9cCMwc0XYScF1E7AVcl5YBZgF7pZ/5wLlQvEkAHwb2B14DfLj2RmFmZp3TNPQj4n+B9SOaZwMXpdsXAUeV2r8ShZ8DEyRNBmYASyJifURsAJbwzDcSMzMbZ1t6Inf3iLgn3b4X2D3dngKsKm23OrU1an8GSfMlDUkaWrdu3RaWZ2Zm9Yz56p2ICCDaUEttf4siYnpETJ80aVK7dmtmZmx56N+Xpm1I/65N7WuAqaXtBlJbo3YzM+ugLQ39xUDtCpy5wFWl9nekq3gOAB5M00DXAodKmphO4B6a2qxP+SuWzaqp6XX6ki4FDgJ2k7Sa4iqchcAVkuYBdwFHp82vAQ4DhoE/Au8EiIj1kj4O3JS2+1hEjDw5bGZm46xp6EfE2xqsOqTOtgEc12A/FwAXjKo6MzNrK38Ng5lZRhz6ZmYZyeK7d3xC0cys4JG+mVlGHPpmZhlx6JuZZSSLOX3rnvL5lJULD+9iJWYGHulbB/lTumbd59C3jnP4m3WPQ9/MLCMOfTOzjDj0res83WPWOQ59M7OM+JJN6xqP7s06z6FvleNr+83Gj0PfKsMjf7Px5zl9q7SRJ3l90tdsbBz6ZmYZ8fSO9QSP7s3aw6FvPWnkm4BP+Jq1xtM71nc872/WmEf61hfqhbyPBsyeyaFv2Whl9O83Buu22ut0vF6Lnt4xq8NTRNavPNI3a4E/JWz9oq9D3yM1Gyu/hqzf9HXom43WaEK+0dzreM/Jmo2FQ99slBq9MYxsr7ed3wis2xz6ZmM0lqODzR0V+IihWpod2ZVV+Xfm0DfrglaOCqz7xvp7qeIbt0PfrOIafcjMVxSNXaMjr1buM9ZtusWhb9ZjRvPp49GMNKs4Ku2UKod0uzn0zfrQaKaPev3qo1br7eaRUZX61KFvlrlmo9xWTlSO91dcjCU0Wzkyyokiots1NDR9+vQYGhra4vvn/Is1q7pW5tFHM9feSzb35tWOowJJSyNier11HumbWVf0W5D3Coe+mVVWv74xdHOO39+yaWaWEY/0zcy6pBtXFHV8pC9ppqQVkoYlndTpxzczy1lHR/qStgbOAd4IrAZukrQ4Im7vZB1mZlXTqfMXnR7pvwYYjojfR8RfgMuA2R2uwcwsW52e058CrCotrwb2L28gaT4wPy0+ImnFGB9zN+D+Me6jU3qpVuitenupVuitenupVuiRenUGsOW1vqDRisqdyI2IRcCidu1P0lCjDylUTS/VCr1Vby/VCr1Vby/VCr1V73jU2unpnTXA1NLyQGozM7MO6HTo3wTsJWmapO2AOcDiDtdgZpatjk7vRMQmSe8BrgW2Bi6IiNvG+WHbNlXUAb1UK/RWvb1UK/RWvb1UK/RWvW2vtdJfuGZmZu3lr2EwM8uIQ9/MLCN9G/pV/7oHSVMl3SDpdkm3STo+te8iaYmkO9K/E7tda42krSX9StLVaXmapBtTH1+eTs5XgqQJkq6U9BtJyyUdWNW+lfS+9Bq4VdKlknaoUt9KukDSWkm3ltrq9qUKZ6e6l0narwK1fiq9DpZJ+pakCaV1C1KtKyTN6GStjeotrTtBUkjaLS23pW/7MvRLX/cwC9gbeJukvbtb1TNsAk6IiL2BA4DjUo0nAddFxF7AdWm5Ko4HlpeWzwDOiog9gQ3AvK5UVd9nge9HxEuBV1DUXbm+lTQF+E9gekS8jOIChzlUq28vBGaOaGvUl7OAvdLPfODcDtVYcyHPrHUJ8LKIeDnwW2ABQPp7mwPsk+7z+ZQdnXQhz6wXSVOBQ4E/lJrb07cR0Xc/wIHAtaXlBcCCbtfVpOarKL6TaAUwObVNBlZ0u7ZUywDFH/fBwNWAKD4puE29Pu9yrTsDd5IuVCi1V65v+eun1HehuJruamBG1foWGARubdaXwBeAt9Xbrlu1jlj3j8Al6fbTcoHiqsIDu923qe1KisHKSmC3dvZtX470qf91D1O6VEtTkgaBVwI3ArtHxD1p1b3A7t2qa4TPACcCT6blXYGNEbEpLVepj6cB64Avp+moL0l6NhXs24hYA3yaYkR3D/AgsJTq9m1No76s+t/eu4DvpduVrFXSbGBNRNw8YlVb6u3X0O8ZknYCvgG8NyIeKq+L4u2869fUSnoTsDYilna7lhZtA+wHnBsRrwQeZcRUToX6diLFlw5OA/YAnk2dw/0qq0pfNiPpFIpp1Uu6XUsjkp4FnAx8aLweo19Dvye+7kHSthSBf0lEfDM13ydpclo/GVjbrfpKXgccKWklxTejHkwxZz5BUu0DflXq49XA6oi4MS1fSfEmUMW+fQNwZ0Ssi4jHgW9S9HdV+7amUV9W8m9P0rHAm4Bj0psUVLPWF1EMAG5Of28DwC8lPZ821duvoV/5r3uQJOB8YHlEnFlatRiYm27PpZjr76qIWBARAxExSNGX10fEMcANwFvSZpWoFSAi7gVWSXpJajoEuJ0K9i3FtM4Bkp6VXhO1WivZtyWN+nIx8I50pckBwIOlaaCukDSTYmryyIj4Y2nVYmCOpO0lTaM4QfqLbtRYExG3RMTzImIw/b2tBvZLr+n29G2nT1p08OTIYRRn6n8HnNLteurU93qKQ+JlwK/Tz2EUc+XXAXcAPwR26XatI+o+CLg63X4hxR/JMPB1YPtu11eqc19gKPXvt4GJVe1b4KPAb4BbgYuB7avUt8ClFOcbHk8hNK9RX1Kc4D8n/d3dQnFVUrdrHaaYC6/9nZ1X2v6UVOsKYFYV+nbE+pX89URuW/rWX8NgZpaRfp3eMTOzOhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXk/wFB/R6YnosbZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 길이 분포 확인하기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "for sen in train_data['document']:\n",
    "    length = len(sen)\n",
    "    \n",
    "    # 문장 최소 길이 찾기\n",
    "    if min_len > length: \n",
    "        min_len = length\n",
    "    \n",
    "    # 문장 최대 길이 찾기\n",
    "    if max_len < length: \n",
    "        max_len = length\n",
    "        \n",
    "    # 전체 문장을 합치면 길이가 얼마나 될까요?\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(train_data))\n",
    "\n",
    "# 전체 길이만큼 0벡터 ==> 길이에 따른 문장의 수를 저장하기 위해 먼저 0으로 이루어진 리스트를 만든다!!\n",
    "sentence_length = np.zeros((max_len), dtype=np.int)\n",
    "print(\"sentence_length : \",sentence_length)\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# raw는 위에서 다운로드한 데이터셋!! 전체 길이와 상관없음\n",
    "for sen in train_data['document']:\n",
    "    sentence_length[len(sen)-1] += 1 # 0으로 이루어진 벡터에 문장 count를 더한 뒤 넣는다.\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0) # 너비는 1.0씩 늘어나도록 설정\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01150842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분포를 고려해 50 이하 단어는 제거 \n",
    "train_list = [s for s in train_data['document'] if (len(s) <= 50)]\n",
    "test_list = [s for s in test_data['document'] if (len(s) <= 50)]\n",
    "\n",
    "train_list = list(set(train_list))\n",
    "test_list = list(set(test_list))\n",
    "\n",
    "train_df = pd.DataFrame(train_list)\n",
    "test_df = pd.DataFrame(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da7174bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이 50이하인 데이터를 기존 데이터와 병합합니다.\n",
    "\n",
    "new_train_df = pd.merge(train_data, train_df, how='inner', left_on='document', right_on=0)\n",
    "new_test_df = pd.merge(test_data, test_df, how='inner', left_on='document', right_on=0)\n",
    "\n",
    "train_data = new_train_df[['id', 'document', 'label']]\n",
    "test_data = new_test_df[['id', 'document', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d87e062a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5403919</td>\n",
       "      <td>막 걸음마 뗀 세부터 초등학교 학년생인 살용영화ㅋㅋㅋ별반개도 아까움</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                               document  label\n",
       "0   9976970                      아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312             흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2  10265843                      너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019              교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4   5403919  막 걸음마 뗀 세부터 초등학교 학년생인 살용영화ㅋㅋㅋ별반개도 아까움      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임 돈주고 보기에는</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6723715</td>\n",
       "      <td>만 아니었어도 별 다섯 개 줬을텐데 왜 로 나와서 제 심기를 불편하게 하죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7898805</td>\n",
       "      <td>음악이 주가 된 최고의 음악영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                   document  label\n",
       "0  6270596                                        굳 ㅋ      1\n",
       "1  8544678           뭐야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아      0\n",
       "2  6825595                  지루하지는 않은데 완전 막장임 돈주고 보기에는      0\n",
       "3  6723715  만 아니었어도 별 다섯 개 줬을텐데 왜 로 나와서 제 심기를 불편하게 하죠      0\n",
       "4  7898805                          음악이 주가 된 최고의 음악영화      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련데이터 :  122623\n",
      "테스트데이터 :  40930\n"
     ]
    }
   ],
   "source": [
    "display(train_data.head())\n",
    "display(test_data.head())\n",
    "\n",
    "print(\"훈련데이터 : \",len(train_data))\n",
    "print(\"테스트데이터 : \",len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f8ccb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분리하기 \n",
    "\n",
    "X_train,X_train_word_index, X_train_index_word = sp_tokenize(s, train_data['document'])\n",
    "X_test,X_test_word_index, X_test_index_word = sp_tokenize(s, test_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51936fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 list 상태 ==> ndarray로 바꿔주기\n",
    "y_train = np.array(list(train_data['label']))\n",
    "y_test = np.array(list(test_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed2aeeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 138  106 2662  918 4930    4 4930  839   69  553  517 2630    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   4 7660  465 1773  144   14  440 3232 2763 1800  177  410  394   41\n",
      "  4251    4   11 7571   29 2410  242   69    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "[[   4 7889    4    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   4 7826  165   25 1105  349  107  180 5502  162 4305  645    4 3412\n",
      "    10  106  381 7630 2509    0   89    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:2])\n",
    "print(X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf92645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_input, val_input, train_target, val_target = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0f114cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순환신경망 모델 선언\n",
    "\n",
    "from tensorflow import keras\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(8000,16))\n",
    "model.add(keras.layers.LSTM(8))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5449153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          128000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 128,881\n",
      "Trainable params: 128,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5648df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3066/3066 [==============================] - 79s 25ms/step - loss: 0.6932 - accuracy: 0.5036 - val_loss: 0.6932 - val_accuracy: 0.4975\n",
      "Epoch 2/10\n",
      "3066/3066 [==============================] - 75s 24ms/step - loss: 0.6932 - accuracy: 0.5027 - val_loss: 0.6931 - val_accuracy: 0.5025\n",
      "Epoch 3/10\n",
      "3066/3066 [==============================] - 75s 24ms/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6931 - val_accuracy: 0.5025\n",
      "Epoch 4/10\n",
      "3066/3066 [==============================] - 75s 24ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6932 - val_accuracy: 0.4975\n",
      "Epoch 5/10\n",
      "3066/3066 [==============================] - 75s 24ms/step - loss: 0.6651 - accuracy: 0.5815 - val_loss: 0.6619 - val_accuracy: 0.5905\n",
      "Epoch 6/10\n",
      "3066/3066 [==============================] - 74s 24ms/step - loss: 0.6396 - accuracy: 0.6343 - val_loss: 0.6437 - val_accuracy: 0.6158\n",
      "Epoch 7/10\n",
      "3066/3066 [==============================] - 75s 24ms/step - loss: 0.6895 - accuracy: 0.5104 - val_loss: 0.6929 - val_accuracy: 0.4975\n",
      "Epoch 8/10\n",
      "3066/3066 [==============================] - 74s 24ms/step - loss: 0.6378 - accuracy: 0.6422 - val_loss: 0.6064 - val_accuracy: 0.7023\n",
      "Epoch 9/10\n",
      "3066/3066 [==============================] - 76s 25ms/step - loss: 0.6177 - accuracy: 0.6717 - val_loss: 0.6256 - val_accuracy: 0.6604\n",
      "Epoch 10/10\n",
      "3066/3066 [==============================] - 76s 25ms/step - loss: 0.6246 - accuracy: 0.6616 - val_loss: 0.6243 - val_accuracy: 0.6682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fac2f00e0d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_input, train_target, epochs=10, validation_data=(val_input, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6303059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 - 7s - loss: 0.6241 - accuracy: 0.6657\n",
      "[0.6240721940994263, 0.6657219529151917]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test,  y_test, verbose=2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fe886",
   "metadata": {},
   "source": [
    "[회고]\n",
    "\n",
    "이번 Exp는 이해가 잘 안간다. 다시 봐야 할 듯. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
