{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e23a40a",
   "metadata": {},
   "source": [
    "## 12-14. 프로젝트: 한국어 데이터로 챗봇 만들기\n",
    "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다.\n",
    "\n",
    "### Step 1. 데이터 수집하기\n",
    "\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663f6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud shell에서 아래 명령어를 입력해 주세요.\n",
    "\n",
    "# $ mkdir -p ~/aiffel/transformer_chatbot/data/\n",
    "# $ ln -s ~/data/* ~/aiffel/transformer_chatbot/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09618bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df45d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    questions=[]\n",
    "    answers=[]\n",
    "    with open('data/ChatbotData .csv','r',encoding=\"utf-8\") as f:\n",
    "        for line in f.read().splitlines():\n",
    "            questions.append(line.split(',')[0:1][0])\n",
    "            answers.append(line.split(',')[1:2][0])\n",
    "        questions, answers, test_Q, test_A = train_test_split(questions, answers, test_size=0.05, random_state=42)\n",
    "    return questions, answers, test_Q, test_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7bf7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, test_Q, answers, test_A = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1771e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11232\n",
      "전체 샘플 수 : 11232\n",
      "테스트 샘플 수 : 592\n",
      "테스트 샘플 수 : 592\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))\n",
    "print('테스트 샘플 수 :', len(test_Q))\n",
    "print('테스트 샘플 수 :', len(test_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad44bc2",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 전처리하기\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는  \n",
    "다른 전처리를 수행해야 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17751fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "    # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "    # student와 온점 사이에 거리를 만듭니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1c02b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: 어이 없는 헤어짐\n",
      "전처리 후의 22번째 답변 샘플: 예상하지 못한 이별이었네요 .\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 22번째 질문 샘플: {}'.format(preprocess_sentence(questions[23])))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(preprocess_sentence(answers[23])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdbd7c7",
   "metadata": {},
   "source": [
    "### Step 3. SubwordTextEncoder 사용하기\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌  \n",
    "위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "637eb8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성.\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29173c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20adb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [7841]\n",
      "END_TOKEN의 번호 : [7842]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba8c2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7843\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13d1affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 23번째 질문 샘플: [3268, 7617, 121, 749]\n",
      "정수 인코딩 후의 23번째 답변 샘플: [6324, 122, 1128, 1178, 6373, 7631]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 23번째 질문 샘플: {}'.format(tokenizer.encode(questions[23])))\n",
    "print('정수 인코딩 후의 23번째 답변 샘플: {}'.format(tokenizer.encode(answers[23])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c87207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9926994301994302\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for q in answers: \n",
    "    if len(q) < 40:\n",
    "        temp.append(q)\n",
    "print(len(temp)/len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d107a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이.\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ae45cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [],[]\n",
    "      \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        \n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7aaf153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 7843\n",
      "필터링 후의 샘플 개수: 11232\n",
      "필터링 후의 샘플 개수: 11232\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6064001",
   "metadata": {},
   "source": [
    "### Step 4. 모델 구성하기\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "275df6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e725b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"어텐션 가중치를 계산. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "260c0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다.\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷-프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다.\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5588a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46f4b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7466910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "          d_model, num_heads, name=\"attention\")({\n",
    "              'query': inputs,\n",
    "              'key': inputs,\n",
    "              'value': inputs,\n",
    "              'mask': padding_mask\n",
    "          })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 두번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "          inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10144c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "\t# 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96e8f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "          shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "          d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "              'query': inputs,\n",
    "              'key': inputs,\n",
    "              'value': inputs,\n",
    "              'mask': look_ahead_mask\n",
    "          })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "          d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "              'query': attention1,\n",
    "              'key': enc_outputs,\n",
    "              'value': enc_outputs,\n",
    "              'mask': padding_mask\n",
    "          })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "          inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "          outputs=outputs,\n",
    "          name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c21499e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21fa44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "465b74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크하기위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 두번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "       )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "   # 디코더\n",
    "    dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad109e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3062016     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3589376     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 7843)   2015651     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,667,043\n",
      "Trainable params: 8,667,043\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08863e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    \n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33eb34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e14b3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88b5bb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "176/176 [==============================] - 15s 53ms/step - loss: 1.4508 - accuracy: 0.0292\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 1.1900 - accuracy: 0.0491\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 10s 54ms/step - loss: 1.0176 - accuracy: 0.0501\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 10s 54ms/step - loss: 0.9417 - accuracy: 0.0533\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 10s 54ms/step - loss: 0.8842 - accuracy: 0.0569\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 9s 54ms/step - loss: 0.8272 - accuracy: 0.0606\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 0.7645 - accuracy: 0.0662\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 0.6954 - accuracy: 0.0732\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 0.6203 - accuracy: 0.0818\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 9s 54ms/step - loss: 0.5404 - accuracy: 0.0907\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 10s 54ms/step - loss: 0.4596 - accuracy: 0.1003\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - 9s 54ms/step - loss: 0.3784 - accuracy: 0.1111\n",
      "Epoch 13/20\n",
      "176/176 [==============================] - 9s 54ms/step - loss: 0.3028 - accuracy: 0.1218\n",
      "Epoch 14/20\n",
      "176/176 [==============================] - 9s 54ms/step - loss: 0.2351 - accuracy: 0.1322\n",
      "Epoch 15/20\n",
      "176/176 [==============================] - 9s 54ms/step - loss: 0.1753 - accuracy: 0.1421\n",
      "Epoch 16/20\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 0.1287 - accuracy: 0.1507\n",
      "Epoch 17/20\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 0.0928 - accuracy: 0.1571\n",
      "Epoch 18/20\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 0.0683 - accuracy: 0.1617\n",
      "Epoch 19/20\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 0.0550 - accuracy: 0.1639\n",
      "Epoch 20/20\n",
      "176/176 [==============================] - 9s 54ms/step - loss: 0.0469 - accuracy: 0.1653\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24f44202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe60c51f820>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1448c4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], 'r')\n",
    "plt.title('Model validation accuracy (resnet_50 vs plain_50)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585288f3",
   "metadata": {},
   "source": [
    "### Step 5. 모델 평가하기\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f28d2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없으므로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af9c1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    \n",
    "  # 입력 문장에 대해서 디코더를 동작시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c512d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 입력 : 죽을 것 같아\n",
      "예측 출력 : 아이스크림 먹어보세요\n",
      "-------------------------\n",
      "질문 입력 : 내일 수학여행가!\n",
      "예측 출력 : 목감기 오려나봐요.\n",
      "-------------------------\n",
      "질문 입력 : 정말. 정말 쉽지가 않네. 이럴 땐 어떡해야 할까\n",
      "예측 출력 : 그렇게 하다보면 있었나봐요. 누구나 그럴 수 있어요.\n",
      "-------------------------\n",
      "질문 입력 : 이별후 너무 외로워ㅠ\n",
      "예측 출력 : 마음에 들면 줘보세요.\n",
      "-------------------------\n",
      "질문 입력 : 싸웠을 때 어떻게 해?\n",
      "예측 출력 : 바로 옆에 있을수도 있어요.\n",
      "-------------------------\n",
      "질문 입력 : 내 사랑의 끝은 이별이지만\n",
      "예측 출력 : 사랑하지 않는 것이죠.\n",
      "-------------------------\n",
      "질문 입력 : 어떻게 벌주어야 할까.\n",
      "예측 출력 : 저 주세요.\n",
      "-------------------------\n",
      "질문 입력 : 발 부었어\n",
      "예측 출력 : 변화를 주는 것도 좋겠죠.\n",
      "-------------------------\n",
      "질문 입력 : 썸 타는 것도 귀찮아.\n",
      "예측 출력 : 썸 탈 사람 없어서 못 타요.\n",
      "-------------------------\n",
      "질문 입력 : 좋아하는 애랑 전화하면\n",
      "예측 출력 : 늦은 시간 전화는 실례예요.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('질문 입력 : {}'.format(test_Q[i]))\n",
    "    print('예측 출력 : {}'.format(sentence_generation(test_Q[i])))\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cadc3989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 입력 : 내 사랑의 끝은 이별이지만\n",
      "정답 출력 : 더 좋은 시작이 있을 거예요.\n"
     ]
    }
   ],
   "source": [
    "print('질문 입력 : {}'.format(test_Q[5]))\n",
    "print('정답 출력 : {}'.format(test_A[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfa47c",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d91591",
   "metadata": {},
   "source": [
    "오 무섭다. 이 정도 정확도면 뭔가 이 챗봇과 얘기해도 즐거울 듯. \n",
    "맞지 않는 것처럼 보이는 것도 있지만 사람도 원래 봉창 두드리는 말을 하는 경우도 있으니 이해가 된다.\n",
    "마지막이 압권이다. \"내 사랑의 끝은 이별이지만\"이라고 했더니 \"더 좋은 시작이 있을 거예요\"라는 대답. \n",
    "거의 영화 Her의 한 장면 아닌가. \n",
    "\n",
    "코딩의 어려운 부분 중 하나는 함수의 경우 indent가 잘 못되더라도 쥬피터 셀에서는 실행이 되고, 나중에 그 함수를 호출할 때 \n",
    "에러가 난다는 것이다. \n",
    "\n",
    "그리고 NLP에서 dropout rate를 높이는 것은 좋은 방법이 아닌 것 같다. 0.3으로도 해 봤는데 이상한 대답들이 나온다. \n",
    "\n",
    "Transformer는 이해하기 결코 쉽지 않다. 나중에 논문을 찾아서 읽어 보겠지만 NLP는 순차적 공부가 맞는 것 같다. \n",
    "그리고 앞의 예제를 따라 그냥 하기는 하는데 솔직히 코드가 너무 길고 이해가 안가는 것이 많아서 \n",
    "앞으로 NLP를 잘 할 수 있을지 모르겠다. \n",
    "\n",
    "언젠가 NLP 관련 Exploration을 Pytorch로 다시 구현해야겠다. 어차피 Pytorch로 먹고 살려고 하니. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
